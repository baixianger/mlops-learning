{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. `LightningModule`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A LightningModule organizes your PyTorch code into 6 sections:\n",
    "- Computations (init).\n",
    "- Train Loop (training_step)\n",
    "- Validation Loop (validation_step)\n",
    "- Test Loop (test_step)\n",
    "- Prediction Loop (predict_step)\n",
    "- Optimizers and LR Schedulers (configure_optimizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LightningModule has many convenience methods, but the core ones you need to know about are:\n",
    "|Name|Description|\n",
    "|--|--|\n",
    "|init|Define computations here|\n",
    "|forward|Use for inference only (separate from training_step)|\n",
    "|training_step|the complete training loop|\n",
    "|validation_step|the complete validation loop|\n",
    "|test_step|the complete test loop|\n",
    "|predict_step|the complete prediction loop|\n",
    "|configure_optimizers|define optimizers and LR schedulers|\n",
    "|||\n",
    "|||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from pytorch_lightning.core.lightning import LightningModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Define the basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "class LitMNIST(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(28 * 28, 128)\n",
    "        self.layer_2 = nn.Linear(128, 256)\n",
    "        self.layer_3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, height, width = x.size()\n",
    "\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = F.relu(self.layer_1(x))\n",
    "        x = F.relu(self.layer_2(x))\n",
    "        x = F.log_softmax(self.layer_3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "net = LitMNIST()\n",
    "x = torch.randn(1, 1, 28, 28)\n",
    "out = net(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Add `training_step`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitMNIST(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(28 * 28, 128)\n",
    "        self.layer_2 = nn.Linear(128, 256)\n",
    "        self.layer_3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, height, width = x.size()\n",
    "\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = F.relu(self.layer_1(x))\n",
    "        x = F.relu(self.layer_2(x))\n",
    "        x = F.log_softmax(self.layer_3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Add `configure_optimizers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "class LitMNIST(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(28 * 28, 128)\n",
    "        self.layer_2 = nn.Linear(128, 256)\n",
    "        self.layer_3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, height, width = x.size()\n",
    "\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = F.relu(self.layer_1(x))\n",
    "        x = F.relu(self.layer_2(x))\n",
    "        x = F.log_softmax(self.layer_3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # 因为LightningModule是Module的子类，\n",
    "        # 所以可以用self.parmeters()直接访问\n",
    "        return Adam(self.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 几种情形：\n",
    "#### 1⃣️. most cases. no learning rate scheduler\n",
    "```python\n",
    "def configure_optimizers(self):\n",
    "    return Adam(self.parameters(), lr=1e-3)\n",
    "```\n",
    "#### 2⃣️. multiple optimizer case (e.g.: GAN)\n",
    "```python\n",
    "def configure_optimizers(self):\n",
    "    gen_opt = Adam(self.model_gen.parameters(), lr=0.01)\n",
    "    dis_opt = Adam(self.model_dis.parameters(), lr=0.02)\n",
    "    return gen_opt, dis_opt\n",
    "```\n",
    "#### 3⃣️. example with learning rate schedulers\n",
    "```python\n",
    "def configure_optimizers(self):\n",
    "    gen_opt = Adam(self.model_gen.parameters(), lr=0.01)\n",
    "    dis_opt = Adam(self.model_dis.parameters(), lr=0.02)\n",
    "    dis_sch = CosineAnnealing(dis_opt, T_max=10)\n",
    "    return [gen_opt, dis_opt], [dis_sch]\n",
    "```\n",
    "#### 4⃣️. example with step-based learning rate schedulers and each optimizer has its own scheduler\n",
    "```python\n",
    "def configure_optimizers(self):\n",
    "    gen_opt = Adam(self.model_gen.parameters(), lr=0.01)\n",
    "    dis_opt = Adam(self.model_dis.parameters(), lr=0.02)\n",
    "    gen_sch = {\n",
    "        'scheduler': ExponentialLR(gen_opt, 0.99),\n",
    "        'interval': 'step'  # called after each training step\n",
    "    }\n",
    "    dis_sch = CosineAnnealing(dis_opt, T_max=10) # called every epoch\n",
    "    return [gen_opt, dis_opt], [gen_sch, dis_sch]\n",
    "```\n",
    "#### 5⃣️. example with optimizer frequencies. see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1 https://arxiv.org/abs/1704.00028\n",
    "```python\n",
    "def configure_optimizers(self):\n",
    "    gen_opt = Adam(self.model_gen.parameters(), lr=0.01)\n",
    "    dis_opt = Adam(self.model_dis.parameters(), lr=0.02)\n",
    "    n_critic = 5\n",
    "    return (\n",
    "        {'optimizer': dis_opt, 'frequency': n_critic},\n",
    "        {'optimizer': gen_opt, 'frequency': 1}\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Pytorch Dataloader方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import datasets, transforms\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "# transforms\n",
    "# prepare transforms standard to MNIST\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# data\n",
    "from pathlib import Path\n",
    "path_root = os.getcwd()\n",
    "mnist_train = MNIST(os.path.join(str(path_root),'dataset'), train=True, download=True, transform=transform)\n",
    "mnist_train = DataLoader(mnist_train, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass in the dataloaders to the .fit() function directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "Missing logger folder: /Users/baixiang/Desktop/pytorch_lightning_learning/lightning_logs\n",
      "\n",
      "  | Name    | Type   | Params\n",
      "-----------------------------------\n",
      "0 | layer_1 | Linear | 100 K \n",
      "1 | layer_2 | Linear | 33.0 K\n",
      "2 | layer_3 | Linear | 2.6 K \n",
      "-----------------------------------\n",
      "136 K     Trainable params\n",
      "0         Non-trainable params\n",
      "136 K     Total params\n",
      "0.544     Total estimated model params size (MB)\n",
      "/opt/miniconda3/envs/course02456/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81:   9%|▊         | 81/938 [00:02<00:31, 27.54it/s, loss=0.00642, v_num=0]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/course02456/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81:   9%|▊         | 81/938 [00:13<02:27,  5.81it/s, loss=0.00642, v_num=0]"
     ]
    }
   ],
   "source": [
    "model = LitMNIST()\n",
    "trainer = Trainer()\n",
    "trainer.fit(model, mnist_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dc2db73289324c0ce9d4ded898b2305b53dddf0a1554c183932275b0c8b0fb14"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('mlops': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
